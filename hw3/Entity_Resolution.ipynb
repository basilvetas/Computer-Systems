{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package Installation Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip3 install numpy\n",
    "# pip3 install pandas\n",
    "# pip3 install recordlinkage\n",
    "# https://www.scipy.org/install.html\n",
    "# pip3 install -U scikit-learn\n",
    "# https://matplotlib.org/users/installing.html\n",
    "# pip3 install missingno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data analysis \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import recordlinkage\n",
    "\n",
    "# for plotting missing data\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display options\n",
    "# %matplotlib inline\n",
    "# pd.options.display.max_rows = 999\n",
    "# pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable multiple cell outputs\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recordlinkage.preprocessing import clean, phonenumbers\n",
    "\n",
    "def preprocess(df1, df2):\n",
    "        \n",
    "    # set index to the id column\n",
    "    df1 = df1.set_index('id')\n",
    "    df2 = df2.set_index('id')\n",
    "    \n",
    "    # replace empty cells with NaN\n",
    "    df1 = df1.replace(\"\", np.nan)\n",
    "    df2 = df2.replace(\"\", np.nan)\n",
    "        \n",
    "    # drop country, locality and region\n",
    "    df1 = df1.drop(['country', 'locality', 'region'], axis=1)\n",
    "    df2 = df2.drop(['country', 'locality', 'region'], axis=1)\n",
    "\n",
    "    # remove all non-numbers from phone & convert to numeric\n",
    "    df1.loc[:, 'phone'] = pd.to_numeric(phonenumbers(df1.loc[:, 'phone']))\n",
    "    df2.loc[:, 'phone'] = pd.to_numeric(phonenumbers(df2.loc[:, 'phone']))\n",
    "    \n",
    "    # convert postal_code to numeric\n",
    "    df1.loc[:, 'postal_code'] = pd.to_numeric(df1.loc[:, 'postal_code'])\n",
    "    df2.loc[:, 'postal_code'] = pd.to_numeric(df2.loc[:, 'postal_code'])\n",
    "    \n",
    "    # clean street_address & website\n",
    "    df1.loc[:, 'street_address'] = clean(df1.loc[:, 'street_address'])\n",
    "    df1.loc[:, 'website'] = clean(df1.loc[:, 'website'])\n",
    "    \n",
    "    df2.loc[:, 'street_address'] = clean(df2.loc[:, 'street_address'])    \n",
    "    df2.loc[:, 'website'] = clean(df2.loc[:, 'website'])\n",
    "    \n",
    "    # convert NaNs to 0s for numerics\n",
    "    df1.loc[:,['latitude', 'longitude', 'phone', 'postal_code']] = \\\n",
    "    df1.loc[:,['latitude', 'longitude', 'phone', 'postal_code']].replace(np.nan, 0)\n",
    "    \n",
    "    df2.loc[:,['latitude', 'longitude', 'phone', 'postal_code']] = \\\n",
    "    df2.loc[:,['latitude', 'longitude', 'phone', 'postal_code']].replace(np.nan, 0)\n",
    "\n",
    "    return df1, df2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_matches(matches):\n",
    "    \n",
    "    # set multiindex to locu_id and foursquare_id\n",
    "    matches = matches.set_index(['locu_id', 'foursquare_id'])\n",
    "    \n",
    "    # drop matches I disagree with\n",
    "    matches = matches.drop('c170270283ef870d546b', level='locu_id') # foursquare_id: 51eb7eed498e401ec51196b6\n",
    "    matches = matches.drop('496bd5b462f08383d880', level='locu_id') # foursquare_id: 3fd66200f964a5209eea1ee3\n",
    "    matches = matches.drop('9ea3254360d0fe59177e', level='locu_id') # foursquare_id: 4dc597c57d8b14fb462ed076\n",
    "    matches = matches.drop('edeba23f215dcc702220', level='locu_id') # foursquare_id: 51a11cbc498e4083823909f1\n",
    "\n",
    "    # create a dataframe for both fourquare and locu of pairs that get matched\n",
    "#     tuples = list(matches.index)\n",
    "#     locu_index = [i[0] for i in tuples]\n",
    "#     four_index = [i[1] for i in tuples]\n",
    "#     locu_matches = locu_train.loc[locu_index]\n",
    "#     four_matches = four_train.loc[four_index]\n",
    "\n",
    "    # for viewing full match dataset\n",
    "#     temp = matches.reset_index().join(four_matches,on=['foursquare_id'])\n",
    "#     match_pairs = temp.join(locu_matches,on=['locu_id'],lsuffix='_foursquare',rsuffix='_locu').set_index(matches.index.names)\n",
    "    \n",
    "#     cols = np.array(match_pairs.columns.tolist())\n",
    "#     order = [0,7,1,8,2,9,3,10,4,11,5,12,6,13]\n",
    "#     cols = list(cols[order])\n",
    "#     matches_reordered = match_pairs[cols]\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recordlinkage.base import BaseIndexator\n",
    "\n",
    "def index_pairs(df1, df2):\n",
    "    indexer = recordlinkage.FullIndex() # BlockIndex(on='postal_code')\n",
    "    return indexer.index(df1, df2)\n",
    "\n",
    "    # Customer Indexer\n",
    "    # class FirstLetterOfNameIndex(BaseIndexator):\n",
    "    #     \"\"\"Custom class for indexing\"\"\"\n",
    "\n",
    "    #     def __init__(self, letter):\n",
    "    #         super(FirstLetterOfNameIndex, self).__init__()\n",
    "\n",
    "    #         # the letter to save\n",
    "    #         self.letter = letter\n",
    "\n",
    "    #     def _link_index(self, df_a, df_b):\n",
    "    #         \"\"\"Make record pairs that agree on the first letter of the given name.\"\"\"\n",
    "\n",
    "    #         # Select records with names starting with a 'letter'.\n",
    "    #         a_startswith_w = df_a[df_a['name'].str.startswith(self.letter) == True]\n",
    "    #         b_startswith_w = df_b[df_b['name'].str.startswith(self.letter) == True]\n",
    "\n",
    "    #         # Make a product of the two numpy arrays\n",
    "    #         return pd.MultiIndex.from_product(\n",
    "    #             [a_startswith_w.index.values, b_startswith_w.index.values],\n",
    "    #             names=[df_a.index.name, df_b.index.name]\n",
    "    #         )\n",
    "\n",
    "    # for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "    #     indexer = FirstLetterOfNameIndex(letter)\n",
    "    #     candidate_pairs = candidate_pairs | indexer.index(locu_train, four_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_strings(df1, df2, cand_pairs):\n",
    "\n",
    "    compare = recordlinkage.Compare()\n",
    "\n",
    "    # initialise similarity measurement algorithms\n",
    "    # compare.string('country', 'country', method='levenshtein', label='country')\n",
    "    # compare.string('locality', 'locality', method='levenshtein', label='locality')\n",
    "    compare.geo('latitude', 'longitude', 'latitude', 'longitude', scale=1, label='geo_coord')\n",
    "    compare.string('name', 'name', method='levenshtein', label='name')\n",
    "    compare.numeric('phone', 'phone', scale=1, label='phone')\n",
    "    compare.numeric('postal_code', 'postal_code', scale=1, label='postal_code')\n",
    "    # compare.string('region', 'region', method='levenshtein', label='region')\n",
    "    compare.string('street_address', 'street_address', method='levenshtein', label='street_address')\n",
    "    compare.string('website', 'website', method='levenshtein', label='website')\n",
    "\n",
    "    # compute similarity measurements\n",
    "    return compare.compute(cand_pairs, df1, df2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# parameters: \n",
    "#   x_all, a data_frame of all your comparison vectors\n",
    "#   y_all_matches, a data_frame with 0 columns and multiindexed with the matching pairs\n",
    "# returns:\n",
    "#   x_train, a subset of x_all that will be used for model training\n",
    "#   y_train_matches_index, a multiindex object of the the matching pairs\n",
    "def traintestsplit(x_all, y_all_matches):\n",
    "    \n",
    "    tuples = list(y_all_matches.index)\n",
    "    y_matches_index = pd.MultiIndex.from_tuples(tuples, names=['locu_id', 'foursquare_id'])\n",
    "    \n",
    "    x_train = x_all.sample(frac=.9, random_state=158)\n",
    "    y_train_matches_index = x_train.index & y_matches_index\n",
    "#     x_train_features = x_train.loc[y_matches_index]\n",
    "#     x_train_features\n",
    "    \n",
    "    return x_train, y_train_matches_index, y_matches_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "#Predict the match status for all record pairs\n",
    "# parameters:\n",
    "#   x, a data_frame of all your comparison vectors\n",
    "# returns:\n",
    "#   results_index, a multiindex object of predicted matches\n",
    "###\n",
    "def predict(x, model):    \n",
    "    results_index = model.predict(x).set_names(['locu_id', 'foursquare_id'])\n",
    "    return results_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_matches(locu_train_path, foursquare_train_path, matches_train_path, locu_test_path, foursquare_test_path):\n",
    "    four_train = pd.read_json(foursquare_train_path)\n",
    "    locu_train = pd.read_json(locu_train_path) \n",
    "\n",
    "    four_test = pd.read_json(foursquare_test_path)\n",
    "    locu_test = pd.read_json(locu_test_path)\n",
    "\n",
    "    matches_train = pd.read_csv(matches_train_path)\n",
    "    \n",
    "    # visualize missing data\n",
    "#     msno.matrix(four_train)\n",
    "#     msno.matrix(locu_train)\n",
    "#     msno.matrix(four_test)\n",
    "#     msno.matrix(locu_test)\n",
    "    \n",
    "    locu_train, four_train = preprocess(locu_train, four_train)\n",
    "    locu_test, four_test = preprocess(locu_test, four_test)\n",
    "    matches_train = preprocess_matches(matches_train)\n",
    "    \n",
    "    candidate_pairs = index_pairs(locu_train, four_train)\n",
    "    test_candidate_pairs = index_pairs(locu_test, four_test)\n",
    "#     print (len(locu_train), len(four_train), len(candidate_pairs))\n",
    "#     print (len(locu_test), len(four_test), len(test_candidate_pairs))\n",
    "    \n",
    "    features = compare_strings(locu_train, four_train, candidate_pairs)\n",
    "    test_features = compare_strings(locu_test, four_test, test_candidate_pairs)\n",
    "    \n",
    "#     features = features.loc[features['street_address'] > .1]\n",
    "#     features = features.loc[features['name'] > .1]\n",
    "\n",
    "    train_pairs, train_matches_index, all_matches_index = traintestsplit(features, matches_train)\n",
    "    \n",
    "    # Train Logistic Regression classifier\n",
    "    logreg = recordlinkage.LogisticRegressionClassifier()\n",
    "    logreg.learn(train_pairs, train_matches_index)\n",
    "#     print (\"LogReg Intercept: \", logreg.intercept)\n",
    "#     print (\"LogReg Coefficients: \", logreg.coefficients)\n",
    "\n",
    "    # Train SVM classifier\n",
    "    svm = recordlinkage.SVMClassifier()\n",
    "    svm.learn(train_pairs, train_matches_index)\n",
    "    \n",
    "    # Predict on training data with both classifiers\n",
    "    svm_results_index = predict(features, svm)\n",
    "    logreg_results_index = predict(features, logreg)\n",
    "    \n",
    "    # To view pairs\n",
    "#     features.index = features.index.rename(['locu_id', 'foursquare_id'])\n",
    "#     train_matches = features.loc[svm_results_index]\n",
    "#     train_matches\n",
    "    \n",
    "    # Training results     \n",
    "    svm_confn_matrix = recordlinkage.confusion_matrix(all_matches_index, svm_results_index, len(features))\n",
    "#     print(\"SVM Confusion Matrix: \", svm_confn_matrix)\n",
    "#     print(\"SVM Precision: \", recordlinkage.precision(svm_confn_matrix))\n",
    "#     print(\"SVM Recall:    \", recordlinkage.recall(svm_confn_matrix))\n",
    "#     print(\"SVM Accuracy:  \", recordlinkage.accuracy(svm_confn_matrix))\n",
    "#     print(\"SVM F1 Score:  \", recordlinkage.fscore(svm_confn_matrix))\n",
    "    \n",
    "    logreg_confn_matrix = recordlinkage.confusion_matrix(all_matches_index, logreg_results_index, len(features))\n",
    "#     print(\"Logistic Regression Confusion Matrix: \", logreg_confn_matrix)\n",
    "#     print(\"Logistic Regression Precision: \", recordlinkage.precision(logreg_confn_matrix))\n",
    "#     print(\"Logistic Regression Recall:    \", recordlinkage.recall(logreg_confn_matrix))\n",
    "#     print(\"Logistic Regression Accuracy:  \", recordlinkage.accuracy(logreg_confn_matrix))\n",
    "#     print(\"Logistic Regression F1 Score:  \", recordlinkage.fscore(logreg_confn_matrix))\n",
    "    \n",
    "    # Predict on test data with SVM\n",
    "    test_results_index = predict(test_features, svm)\n",
    "    \n",
    "    # Format and write to CSV    \n",
    "    test_features.index = test_features.index.rename(['locu_id', 'foursquare_id'])\n",
    "    test_match_pairs = test_features.loc[test_results_index]\n",
    "    matches_test = test_match_pairs.drop(test_match_pairs.columns[::], axis=1)        \n",
    "#     matches_test\n",
    "    matches_test.to_csv('matches_test.csv')\n",
    "    \n",
    "    # create a dataframe for both fourquare and locu of pairs that get matched\n",
    "    test_tuples = list(matches_test.index)\n",
    "    test_locu_index = [i[0] for i in test_tuples]\n",
    "    test_four_index = [i[1] for i in test_tuples]\n",
    "    test_locu_matches = locu_test.loc[test_locu_index]\n",
    "    test_four_matches = four_test.loc[test_four_index]\n",
    "\n",
    "    # for viewing full match dataset\n",
    "    temp = matches_test.reset_index().join(test_four_matches,on=['foursquare_id'])\n",
    "    test_match_pairs = temp.join(test_locu_matches,on=['locu_id'],lsuffix='_foursquare',rsuffix='_locu').set_index(matches_test.index.names)\n",
    "\n",
    "    cols = np.array(test_match_pairs.columns.tolist())\n",
    "    order = [0,7,1,8,2,9,3,10,4,11,5,12,6,13]\n",
    "    cols = list(cols[order])\n",
    "    test_matches_reordered = test_match_pairs[cols]\n",
    "#     display(test_matches_reordered)    \n",
    "#     print(\"Successfully wrote results to matches_test.csv\")\n",
    "    return\n",
    "        \n",
    "get_matches(\"locu_train_hard.json\", \"foursquare_train_hard.json\", \"matches_train_hard.csv\", \"locu_test_hard.json\", \"foursquare_test_hard.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
