\documentclass[12pt]{article}
 
\usepackage[a4paper]{geometry}
\usepackage[myheadings]{fullpage}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{graphicx, wrapfig, subcaption, setspace, booktabs}
\usepackage[T1]{fontenc}
\usepackage[font=small, labelfont=bf]{caption}
\usepackage{fourier}
\usepackage[protrusion=true, expansion=true]{microtype}
\usepackage[english]{babel}
\usepackage{sectsty}
\usepackage{url, lipsum}
\usepackage{float}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{mathtools}
\usepackage{tikz}
\usepackage{qtree}
\usepackage{hyperref}
\usepackage{listings}
\usepackage[titletoc,toc,title]{appendix}
\usepackage{titlesec}

% Define JavaScript
\usepackage{color}

\definecolor{lightgray}{rgb}{.9,.9,.9}
\definecolor{darkgray}{rgb}{.4,.4,.4}
\definecolor{purple}{rgb}{0.65, 0.12, 0.82}

\lstdefinelanguage{JavaScript}{
  keywords={typeof, new, true, false, catch, function, return, null, catch, switch, var, if, in, while, do, else, case, break},
  keywordstyle=\color{blue}\bfseries,
  ndkeywords={class, export, boolean, throw, implements, import, this},
  ndkeywordstyle=\color{darkgray}\bfseries,
  identifierstyle=\color{black},
  sensitive=false,
  comment=[l]{//},
  morecomment=[s]{/*}{*/},
  commentstyle=\color{purple}\ttfamily,
  stringstyle=\color{red}\ttfamily,
  morestring=[b]',
  morestring=[b]"
}

\lstset{
  language=JavaScript,
  backgroundcolor=\color{lightgray},
  extendedchars=true,
  basicstyle=\footnotesize\ttfamily,
  showstringspaces=false,
  showspaces=false,
  numbers=left,
  numberstyle=\footnotesize,
  numbersep=9pt,
  tabsize=2,
  breaklines=true,
  showtabs=false,
  captionpos=b
}

% Make links blue
\hypersetup{colorlinks,urlcolor=blue}

% Allows use of caligraphy letters
\usepackage{calrsfs}
\DeclareMathAlphabet{\pazocal}{OMS}{zplm}{m}{n}
 
%\usepackage{stackengine}
%\newcommand\letvdash[1]{\mathrel{
  %\stackengine{1ex}{\vdash}{\;\;\scriptscriptstyle#1}{O}{c}{F}{T}{L}}}
%\stackMath

%\newcommand\letvudash[1]{\mathrel{\stackengine{.4ex}{\vdash}‌​{\;\;\scriptscriptst‌​yle#1}{U}{c}{F}{T}{L‌​}}}
 
 
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
 
\newtheorem{p}{}
\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\setlength\parindent{24pt}

% Add a page break between sections
\let\oldsection\section
\renewcommand\section{\clearpage\oldsection}
\renewcommand{\thesection}{Step \arabic{section}}
\renewcommand{\thesubsection}{\arabic{section}.\arabic{subsection}}

\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{reflection}[2][Reflection]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{proposition}[2][Proposition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

% Get variance, argmin and argmax commands
\newcommand{\Var}{\mathrm{Var}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
 
\begin{document}
 
% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------

\title{HW \#1}%replace X with the appropriate number
\author{Basil Vetas (bsv2111)\\ %replace with your name
Computer Systems for Data Science (Spring 2018)\\
W4121 - Columbia University} %if necessary, replace with your course title
\maketitle
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

% JSON
\section{Look at some JSON-encoded Tweets}

\includegraphics[scale=0.65]{twitter_bot.png}
\\Source: https://xkcd.com/1646/

\paragraph{Questions}

\subparagraph{1. Find the number of deleted messages in the dataset.}
\subparagraph{2. Find the number of tweets that are replies to another tweet.}
\subparagraph{3. Find the five user IDs (field name: uid) that have tweeted the most.}

% Protocol Buffers
\section{Analyses using Protocol Buffers} ~
Using protocol buffers I found the number of deleted tweets to be 1554, the number of tweets that are replies to another tweet to be 2531, and the five user IDs that have tweeted the most to be 1269521828 (5 tweets), 392695315 (4 tweets), 424808364 (3 tweets), 1706901902 (3 tweets), 1471774728 (2 tweets). Note that the 5th place user ID is not unique as there were many users with 2 tweets, so I have listed the one returned by my query.

\lstinputlisting[language=Python]{step2.py}

% SQLite
\section{Analyses on database records} ~
Using SQL I found the number of deleted tweets to be 1554, the number of tweets that are replies to another tweet to be 2531, and the five user IDs that have tweeted the most to be 1269521828 (5 tweets), 392695315 (4 tweets), 424808364 (3 tweets), 1706901902 (3 tweets), 23991910 (2 tweets). Note that the 5th place user ID is not unique as there were many users with 2 tweets, so I have listed the one returned by my query.

\lstinputlisting[language=SQL]{step3.sql}

% MongoDB
\section{Analyses in MongoDB} ~
Using MongoDB I found the number of deleted tweets to be 1554, the number of tweets that are replies to another tweet to be 2531, and the five user IDs that have tweeted the most to be 1269521828 (5 tweets), 392695315 (4 tweets), 424808364 (3 tweets), 1706901902 (3 tweets), 578015986 (2 tweets). Note that the 5th place user ID is not unique as there were many users with 2 tweets, so I have listed the one returned by my query.

\lstinputlisting[language=JavaScript]{step4.js}

% Reflection Questions
\section*{Reflection Questions}

\paragraph{1. Read the schema and protocol buffer definition files. What are the main differences between the two? Are there any similarities?}\mbox{}\\ 

The main differences between the schema file, twitter.ddl, and the protocol butter definition file, twitter.proto, are firstly that twitter.ddl makes use of id's, which allows for distinct relational tables and a schema that is very human-interpretable. In contrast, the twitter.proto file seems more hierarchical by making use of nested struct-like definitions, and enforcing them via 'required,' 'repeated,' and 'optional' tags. Otherwise, they are relatively similar in terms of the categories and elements they have used to decompose twitter.json.

\paragraph{2. Describe one question that would be easier to answer with protocol buffers than via a SQL query.}\mbox{}\\

In terms of protocol buffers and SQL, the main tradeoff I see is between flexibility and convenience.  SQL is much more convenient for common queries because a lot has been abstracted away, however, you sacrifice some of the flexibility to implement your own query logic.  Alternatively, protocol buffers are much less convenient and require more work to make common queries, but are very flexible at allowing you to write logic to query anything you need.  Therefore one question that would be easier to answer with protocol buffers than via a SQL query in regard to our tweet data might be something like trying to find the original tweet in a chain conversation, which would require finding the value listed in the "reply to" category for the first tweet, and then going to find that next tweets "reply to" value, and then the next tweet's "reply to" value until you find a tweet with no "reply to".  A query in SQL like this would require complex operations, but with python you could do it relatively easily either iteratively or recursively.    

\paragraph{3. Describe one question that would be easier to answer with MongoDB than via a SQL query.}\mbox{}\\

Questions related to inherently heirarchical data I would expect to bea easier to answer with MongoDB than via SQL query. With the twitter data we don't have very strong natural heirarchies in the data, but you can imagine with deeply nested data that MongoDB is easier to query deep within a record because of the json-like structure of the records and the nested nature of MongoDB query language. I guess for the twitter data, where MongoDB might be advantages is what we exhibited with the assignment—we didn't require a specific data schema and database structure in order to queries large records.  With MongoDB we basically just dumped twitter.json into the database and made queries on it, this is not feasible with SQL because of the structure and schema required in advance. 

\paragraph{4. Describe one question that would be easier to answer via a SQL query than using MongoDB.}\mbox{}\\

Any type of question related to inherently relational data is going to be easier with SQL using joins that it would be with MongoDB. For instance, a query to find all of the tweets that mention a specific location would require joining tables between location and tweets, which would be much easier with an SQL query than with MongoDB. 

\paragraph{5. What fields in the original JSON structure would be difficult to convert to relational database schemas?}\mbox{}\\

Referring back to my example in question two, I think representing a chain conversation of tweets in a relational database would be very difficult because you would have the "reply to" for one tweet refer to the "reply to" for another tweet and so on without a well defined way of knowing how long the chain is. This seems like it could be both computationally expensive and difficult to implement queries on. 

\paragraph{6. In terms of lines of code, when did various approaches shine? Think about the challenges of defining schemas, loading and storing the data, and running queries.}\mbox{}\\

In terms of lines of code, MongoDB seems to shine in terms of defining schemas, loading and storing data, given that it did not require any specific scripts or config files to do these things. The downside of MongoDB is that the data is less structured for queries. SQL, however, definitely shines in terms of the queries for this assignment, and is probably second best in terms of defining schemas, loading and storing data, requiring just the twitter.ddl schema and createdb.py. Lastly, the protocol buffer required the most work to define the schema, load, store and query data given that it required twitter.proto, encode.py as well as my own script to do each of these, respectively.   

\paragraph{7. What other metrics (e.g., time to implement, code redundancy, etc.) can we use to compare these different approaches? Which system is better by those measures?}\mbox{}\\

Obviouly lines of code is a decent metric. Given that we were provided a lot of the setup schemas and scripts, it is difficult to assess the time to implement if starting from scratch, however for the queries alone SQL was definitely the quickest by time to implement. For code redundancy, because of the fact that the protocol buffer queries required similar code to the encode.py file, I would have to say these were the worst in terms of code redundancy. Neither SQL or MongoDB seem to have too much code redundancy.  Finally, another metric that I think is important but sometimes overlooked when evaluating programming languages in general is the developer community and documentation available online. For these, SQL definitely shines because of its widespread use, however MongoDB I felt was also quite well documented, despite probably having a smaller developer community. Finally, in terms of dev community and documentation, protocol buffers were definitely the worst.

\paragraph{8. How long did this lab take you? We want to make sure to target future labs to not take too much of your time.}\mbox{}\\

The VM setup took two to three hours unfortunately because some of the instructions were outdated and I had to figure out how to edit the Vagrantfile myself to make some adjustments. Otherwise, the protocol buffer portion took a couple of hours because I had to learn the documentation.  The SQL portion was quite quick for me, probably under an hour, and finally the MongoDB took maybe a couple of hours because I haven't used their query language before and there was not a clear schema file like the first two provided. Lastly, to create a nice LaTex file and write up these reflection questions took about two hours. Overall the amount of coding work was pretty ideal, while the setup and writeup portions took more time than I would have liked but now that it is done it should be quicker next time.

\end{document} 



